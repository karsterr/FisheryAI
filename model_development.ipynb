{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CvdwAsN6PD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ae7aa6f-68b0-47d4-d798-4fb2caa3faca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. GOOGLE COLAB: EĞİTİM VE PAKETLEME\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "# GPU'yu sadece eğitim için kullan, ama modeli CPU uyumlu yap\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "# Gerekli Kütüphaneler\n",
        "try:\n",
        "    import kagglehub\n",
        "except ImportError:\n",
        "    os.system('pip install kagglehub')\n",
        "    import kagglehub\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, Concatenate, Embedding, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AYARLAR ---\n",
        "SEQ_LEN = 5\n",
        "FILES_CONFIG = {\n",
        "    'capture-fishery-production.csv': {'type_id': 0, 'col_keyword': 'Capture', 'topic': 'avcılık üretimi'},\n",
        "    'aquaculture-farmed-fish-production.csv': {'type_id': 1, 'col_keyword': 'Aquaculture', 'topic': 'yetiştiricilik'},\n",
        "    'fish-and-seafood-consumption-per-capita.csv': {'type_id': 2, 'col_keyword': 'Food supply', 'topic': 'tüketim'},\n",
        "    'fish-stocks-within-sustainable-levels.csv': {'type_id': 3, 'col_keyword': 'sustainable levels', 'topic': 'stok sürdürülebilirliği'}\n",
        "}"
      ],
      "metadata": {
        "id": "r0bXPimL-CKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VERİ İNDİRME VE İŞLEME ---\n",
        "print(\"Veri seti indiriliyor...\")\n",
        "path = kagglehub.dataset_download(\"sergegeukjian/fish-and-overfishing\")\n",
        "all_files = glob.glob(os.path.join(path, \"**\", \"*.csv\"), recursive=True)\n",
        "\n",
        "X_values, X_types, Y_texts = [], [], []\n",
        "\n",
        "for fname, config in FILES_CONFIG.items():\n",
        "    found = next((f for f in all_files if fname in f), None)\n",
        "    if not found: continue\n",
        "\n",
        "    df = pd.read_csv(found)\n",
        "    try: val_col = [c for c in df.columns if config['col_keyword'] in c][0]\n",
        "    except: continue\n",
        "\n",
        "    df = df.rename(columns={'Entity': 'Entity', 'Year': 'Year', val_col: 'Value'})\n",
        "    df = df.dropna(subset=['Value']).sort_values(['Entity', 'Year'])\n",
        "    df['Value'] = pd.to_numeric(df['Value'], errors='coerce')\n",
        "\n",
        "    for country in df['Entity'].unique():\n",
        "        vals = df[df['Entity'] == country]['Value'].values\n",
        "        if len(vals) < SEQ_LEN + 1: continue\n",
        "\n",
        "        for i in range(len(vals) - SEQ_LEN):\n",
        "            window = vals[i : i+SEQ_LEN]\n",
        "            v_start, v_end = window[0] + 1e-5, window[-1]\n",
        "            pct = ((v_end - v_start) / v_start) * 100\n",
        "\n",
        "            trend = \"artış\" if pct > 10 else \"düşüş\" if pct < -10 else \"stabil\"\n",
        "            if config['type_id'] == 3: # Stoklar\n",
        "                 trend = \"kritik\" if v_end < 60 else \"riskli\" if pct < -5 else \"sürdürülebilir\"\n",
        "\n",
        "            text = f\"start {config['topic']} verileri {trend} seyretti end\"\n",
        "            X_values.append(window); X_types.append(config['type_id']); Y_texts.append(text)"
      ],
      "metadata": {
        "id": "mNhenDeJAmLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d65ccd4-0d13-4806-e490-e5297f07fd56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veri seti indiriliyor...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/sergegeukjian/fish-and-overfishing?dataset_version_number=1202...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516k/516k [00:00<00:00, 662kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PREPROCESSING ---\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(np.array(X_values).T).T.reshape(-1, SEQ_LEN, 1)\n",
        "\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(Y_texts)\n",
        "Y_pad = pad_sequences(tokenizer.texts_to_sequences(Y_texts), padding='post')\n",
        "max_len = Y_pad.shape[1]"
      ],
      "metadata": {
        "id": "RqAOIm0IAocc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL (GPU + TFLITE DOSTU) ---\n",
        "input_seq = Input(shape=(SEQ_LEN, 1), name='ts_input')\n",
        "# unroll=True sayesinde GPU kullanırız ama TFLite hata vermez\n",
        "lstm_out = LSTM(64, return_state=False, unroll=True)(input_seq)\n",
        "input_type = Input(shape=(1,), name='type_input')\n",
        "type_vec = Flatten()(Embedding(4, 16)(input_type))\n",
        "concat = Concatenate()([lstm_out, type_vec])\n",
        "decoder = LSTM(64, return_sequences=True, unroll=True)(RepeatVector(max_len)(Dense(64, activation='relu')(concat)))\n",
        "output = Dense(len(tokenizer.word_index) + 1, activation='softmax')(decoder)\n",
        "\n",
        "model = Model([input_seq, input_type], output)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit([X_scaled, np.array(X_types)], Y_pad, epochs=15, batch_size=64, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox7dh8jVAuTA",
        "outputId": "01398803-616d-46f9-b974-2de46bdda5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - accuracy: 0.6634 - loss: 1.1206\n",
            "Epoch 2/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9714 - loss: 0.0785\n",
            "Epoch 3/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0680\n",
            "Epoch 4/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9719 - loss: 0.0703\n",
            "Epoch 5/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0618\n",
            "Epoch 6/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9739 - loss: 0.0610\n",
            "Epoch 7/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9740 - loss: 0.0610\n",
            "Epoch 8/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9739 - loss: 0.0604\n",
            "Epoch 9/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9735 - loss: 0.0598\n",
            "Epoch 10/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.0587\n",
            "Epoch 11/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9745 - loss: 0.0590\n",
            "Epoch 12/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9734 - loss: 0.0621\n",
            "Epoch 13/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9737 - loss: 0.0593\n",
            "Epoch 14/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9742 - loss: 0.0590\n",
            "Epoch 15/15\n",
            "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.0579\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79f51ee84e30>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DIŞA AKTARMA ---\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('fishery_model.tflite', 'wb') as f: f.write(tflite_model)\n",
        "with open('model_meta.json', 'w') as f:\n",
        "    json.dump({'max_len': max_len, 'tokenizer_json': tokenizer.to_json(), 'topic_map': {v['topic']: v['type_id'] for k, v in FILES_CONFIG.items()}}, f)\n",
        "\n",
        "print(\"\\nBAŞARILI! 'fishery_model.tflite' ve 'model_meta.json' dosyalarını indir.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCR7Mfo0rBci",
        "outputId": "7ba2ce1d-66a1-4226-c128-7b64043bd55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpsbbbqj1k'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='ts_input'), TensorSpec(shape=(None, 1), dtype=tf.float32, name='type_input')]\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 7, 16), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134093692416848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093692417424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093692418960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093692417232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093692419344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093692419920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093667599568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093667599184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093667600144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093692419728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134093667598992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "\n",
            "BAŞARILI! 'fishery_model.tflite' ve 'model_meta.json' dosyalarını indir.\n"
          ]
        }
      ]
    }
  ]
}